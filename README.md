# Internship Scraping and Recommendation System

This repository collects internships from multiple sources, normalizes the data into a single CSV, and exposes a chatbot interface that can search and recommend internships. The chatbot uses a fast local search (rule-based filters / TF‚ÄëIDF ‚Äî planned) as the primary method and Gemini (optional) for friendly, explanatory responses.

## Current project state (what this repo actually contains)

- Orchestration: `orchestrator.py` ‚Äî runs scrapers (currently uses `JobSpy/main.py`), merges results, deduplicates, and writes `data/internships.csv` and a summary file.
- Multi-source scraping: `JobSpy/` (packaged local copy) and an included Internshala scraper in `Internshala-Web-Scraper-Internshala.com/` (these may be present as embedded repos/submodules).
- Chatbot backend: `chatbot/` ‚Äî rule-based `InternshipBot`, CLI (`chatbot/cli.py`), AI wrapper `AIEnhancedInternshipBot` with safe timeouts.
- Data model: `models/internship.py` ‚Äî unified dataclass schema + deduplication helper.
- AI starter: `start_gemini_chatbot.py` ‚Äî loads `.env` automatically (prefer `python-dotenv`) and starts either the AI-enhanced bot or the rule-based bot.
- Data: `data/internships.csv` (generated by orchestrator) and `jobs.csv` (JobSpy output used during orchestration).

## üìÅ Simplified project structure

```
placement_portal/
‚îú‚îÄ‚îÄ JobSpy/                             # JobSpy package (local copy)
‚îú‚îÄ‚îÄ Internshala-Web-Scraper-Internshala.com/  # Original internshala script (embedded)
‚îú‚îÄ‚îÄ chatbot/                             # Chatbot implementation and CLI
‚îú‚îÄ‚îÄ models/                              # Data models (Internship dataclass)
‚îú‚îÄ‚îÄ data/                                # Generated CSVs (internships.csv, jobs.csv)
‚îú‚îÄ‚îÄ orchestrator.py                      # Run scrapers, merge, dedupe, write CSV
‚îú‚îÄ‚îÄ start_gemini_chatbot.py              # Start AI-enhanced chatbot (auto loads .env)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## üõ† Installation (quick)

1. Create virtual environment and activate it:

```bash
python3 -m venv venv
source venv/bin/activate
```

2. Install dependencies:

```bash
pip install -r requirements.txt
# optional: for .env loading and Playwright
pip install python-dotenv
pip install playwright
playwright install
```

3. Put your Gemini key into `.env` (optional):

```bash
echo "GEMINI_API_KEY=your_api_key_here" >> .env
```

## üöÄ Quick Start ‚Äî run the full flow

1. Run the orchestrator (this will run JobSpy/main.py and produce `data/internships.csv`):

```bash
python orchestrator.py --sources jobspy --max-results 50
```

2. Start the chatbot (will auto-load `.env` for GEMINI_API_KEY):

```bash
python start_gemini_chatbot.py --csv data/internships.csv --ai-timeout 15 --verbose
```

Or use the rule-based CLI:

```bash
python -m chatbot.cli
```

## üí¨ Usage examples

- `Show me AI/ML internships in Bangalore`
- `List remote internships with Python and machine learning`
- `parse resume path/to/resume.pdf` then `recommend internships based on my profile`

The system will first try a fast, local search (rule-based or TF‚ÄëIDF if enabled). Gemini (if configured) is used for nicer explanations and personalization but is optional and configured via `GEMINI_API_KEY`.

## üîß Notes & troubleshooting

- If `data/internships.csv` is empty:

  - Run `python orchestrator.py --sources jobspy --max-results 50` and inspect `jobs.csv` created by JobSpy.
  - Some sources may block requests or require JS rendering ‚Äî consider enabling Playwright for those sources.

- Rate limiting: You may see HTTP 429 from some providers (ZipRecruiter etc.). The orchestrator will still proceed with other sources. Use delays, rotate proxies, or reduce `results_wanted`.

- AI timeouts: AI calls are limited (default 10s) and configurable with `--ai-timeout`. If Gemini is slow or unavailable the bot falls back to local results.

## ‚úÖ Recommended next improvements

1. Add a TF‚ÄëIDF or BM25 local index for fast, high-quality candidate retrieval (planned; I can implement this). This will make the chatbot respond sub-second for most queries and drastically reduce reliance on Gemini for basic retrieval.
2. Convert interactive scrapers into reusable modules and add Playwright for JS-heavy sites.
3. Normalize stipend and date fields during orchestration.

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Add your scraper or improvements
4. Test locally (run `orchestrator.py --dry-run` and verify `data/internships.csv`)
5. Submit a pull request

---

If you want, I can implement the TF‚ÄëIDF search + caching next and wire it into the chatbot so queries return instantly ‚Äî say the word and I'll make the change and run a demo.
